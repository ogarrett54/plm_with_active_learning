{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a496b305",
   "metadata": {},
   "source": [
    "# Active Fine-tuning of PLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743064de",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Trial three approaches for active fine-tuning of ESM2 on our Pikh1 HMA data\n",
    "    1. train an ensemble of models and use the mean and variance of their predictions to guide learning\n",
    "    2. use dropout layer during prediction, find mean and variance of predictions\n",
    "    3. mean variance estimation fine-tuning to predict two values with gaussian negative log-likelihood loss\n",
    "- Compare data efficiency between each method (that is, how many training labels are needed to achieve the same spearman r on a universal test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4471d3",
   "metadata": {},
   "source": [
    "## Prepare data splits\n",
    "Starting data is from FACS of surface displayed Pikh1 HMA variants when exposed to 1 uM AVR-PikC. There are 3960 labels in this dataset, each with a full sequence and an enrichment score, roughly correlated with binding affinity. We'll load these into a torch dataset, then split into 80% train, 10% val, and 10% test. The training data will be what we use for all active learning loops, validated on the val set. The test set will be used as a universal final test set for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19d00765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa_sequence</th>\n",
       "      <th>enrichment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...</td>\n",
       "      <td>1.468796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GLKRIIVIKVAREGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...</td>\n",
       "      <td>1.415944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GLKRIIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...</td>\n",
       "      <td>1.389615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...</td>\n",
       "      <td>1.359651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...</td>\n",
       "      <td>1.343857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         aa_sequence  enrichment_score\n",
       "0  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...          1.468796\n",
       "1  GLKRIIVIKVAREGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...          1.415944\n",
       "2  GLKRIIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...          1.389615\n",
       "3  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...          1.359651\n",
       "4  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...          1.343857"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('avrpikC_full.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c9613c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = df.aa_sequence\n",
    "scores = df.enrichment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afccfeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class BindingDataset(Dataset):\n",
    "    def __init__(self, sequences, scores):\n",
    "        # make sure sequence and scores have the same length\n",
    "        assert len(sequences) == len(scores), f\"Sequences and scores must be of the same length.\\nNumber of sequences: {len(sequences)}\\nNumber of scores: {len(scores)}\"\n",
    "        self.sequences = sequences\n",
    "        self.scores = scores\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = torch.tensor(self.scores[idx], dtype=torch.float)\n",
    "\n",
    "        # tokenize the sequence\n",
    "        tokenized = self.tokenizer(\n",
    "            sequence,\n",
    "            max_length=80, # 78 residues + 2 extra tokens\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # return input_ids: attention masks, removing the batch dimension\n",
    "        inputs = {key: val.squeeze(0) for key, val in tokenized.items()}\n",
    "\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99740828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "full_dataset = BindingDataset(sequences, scores)\n",
    "\n",
    "# split the data into train, val, and test sets\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = int(0.1 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "training_pool, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# define dataloaders for val and test sets, train will be defined later for subsets\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3cdc9b",
   "metadata": {},
   "source": [
    "## Approach 1: Ensemble Predictions\n",
    "1. Initialize model 1: pretrained esm2 with a randomized binding head\n",
    "2. Train on an initial batch of 24 samples (test random and zero-shot predictions).\n",
    "    - Initial hyperparameters (note that during an actual active learning run, you can't change this in the middle of the campaign)\n",
    "        - optimizer: AdamW\n",
    "        - learning rate: 2e-5\n",
    "        - weight decay: 0.01\n",
    "        - early stopping\n",
    "3. Evaluate on validation set\n",
    "4. Evalutate on the rest of the available training pool\n",
    "5. Initialize model 2\n",
    "6. Repeat 2-5 until 5 models have been trained and used to evalutate\n",
    "7. Calculate variance of model predictions on each sequence in the training pool.\n",
    "8. Select the next 24 sequences with the highest variance.\n",
    "9. Reinitialize model 1 with pretrained esm2 and a freshly randomized binding head.\n",
    "10. Repeat ensemble training, evaluation, and acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eff210",
   "metadata": {},
   "source": [
    "- Notes about alternative approaches for the baseline fine-tuning approach:\n",
    "    - Fine-tuning ESM2 is still under active research, and recent literature suggests that just using the CLS token for mutant effects may not be the best approach\n",
    "    - [ESM Effect](https://www.biorxiv.org/content/10.1101/2025.02.03.635741v1.full.pdf) details a framework for using the mutant token vectors as the basis for predicting mutant effects.\n",
    "        - Here, they use 35M model with last two layers unfrozen. They use the mutant tokens to predict mutant effects. They have a prediction head composed of two linear layers. However, their datasets are of deep mutational scans, so only one mutation. I would have to adapt this for higher numbers of mutations.\n",
    "            - to adapt it for a higher number of mutations, I could pool, concatenate, or use an attention mechanism\n",
    "- For now, I'm going to set up the active training loop using the standard CLS token approach (aka, what the AutoModelForSequenceClassification class does from the transformers library), but I will need to separately experiment with other approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f789004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.regression import SpearmanCorrCoef\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error() # supress initialization warnings\n",
    "\n",
    "def initialize_HF_ESM2(model_name, learning_rate, weight_decay):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 1)\n",
    "    # loss_fn = MSELoss() : HuggingFace automatically handles the loss\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    spearman = SpearmanCorrCoef()\n",
    "    return model, optimizer, spearman #,loss_fn\n",
    "\n",
    "def train_step(model, optimizer, train_dataloader):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    return avg_train_loss\n",
    "\n",
    "def val_step(model, val_dataloader, spearman):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            preds = outputs.logits.squeeze() # to make sure dimensions are the same for spearman\n",
    "            loss = outputs.loss\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    spearmanr = spearman(all_preds, all_labels).item()\n",
    "\n",
    "    return avg_val_loss, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "143eab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_and_train_new_model(\n",
    "        model_name, \n",
    "        learning_rate, \n",
    "        weight_decay,\n",
    "        epochs, \n",
    "        train_dataloader, \n",
    "        val_dataloader, \n",
    "        patience=5,\n",
    "        return_history=False,\n",
    "        checkpoint_path=\"best_model.pth\"\n",
    "        ):\n",
    "    \n",
    "    model, optimizer, spearman = initialize_HF_ESM2(model_name, learning_rate, weight_decay)\n",
    "\n",
    "    # initialize variables for early stopping\n",
    "    best_val_spearman = -1\n",
    "    epochs_wo_improvement = 0\n",
    "\n",
    "    # initialize lists to store metrics\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    spearmanr_history = []\n",
    "\n",
    "    # main training loop\n",
    "    for epoch in tqdm(range(epochs), desc=\"[Training]\"):\n",
    "        train_loss = train_step(model, optimizer, train_dataloader)\n",
    "        val_loss, spearmanr = val_step(model, val_dataloader, spearman)\n",
    "\n",
    "        train_loss_history.append(train_loss)\n",
    "        val_loss_history.append(val_loss)\n",
    "        spearmanr_history.append(spearmanr)\n",
    "\n",
    "        # early stopping logic\n",
    "        if spearmanr > best_val_spearman:\n",
    "            best_val_spearman = spearmanr\n",
    "            epochs_wo_improvement = 0\n",
    "            # save the best model for later\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "        else:\n",
    "            epochs_wo_improvement += 1\n",
    "        \n",
    "        if epochs_wo_improvement == patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs with no improvement.\")\n",
    "            break\n",
    "        \n",
    "    print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | SpearmanR: {spearmanr:.4f}')\n",
    "\n",
    "    # load the best model before output\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "    if return_history:\n",
    "        history = {\n",
    "            'train_loss': train_loss_history,\n",
    "            'val_loss': val_loss_history,\n",
    "            'spearmanr': spearmanr_history\n",
    "        }\n",
    "        return model, history\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce232e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def acquire_new_batch(dataset, initial_batch_size, batch_size_to_acquire, labeled_indices, unlabeled_indices, acquisition_scores=None):\n",
    "    # if initial batch, when there are no acquisition scores, select randomly\n",
    "    if acquisition_scores is None:\n",
    "        initial_batch_size = min(initial_batch_size, len(unlabeled_indices))\n",
    "        indices_to_acquire = np.random.choice(unlabeled_indices, size=initial_batch_size, replace=False)\n",
    "    \n",
    "    # else select based on top acquisition scores\n",
    "    else:\n",
    "        # make sure we don't overshoot samples to acquire if on the final batch\n",
    "        batch_size_to_acquire = min(batch_size_to_acquire, len(acquisition_scores))\n",
    "        # get the indicies of the top acquisition scores (num of samples)\n",
    "        top_k_indices = acquisition_scores.topk(batch_size_to_acquire).indices\n",
    "        # use these to find the indicies that map back to the original dataset\n",
    "        indices_to_acquire = unlabeled_indices[top_k_indices.cpu().numpy()]\n",
    "    \n",
    "    # update the indices lists\n",
    "    labeled_indices = np.concatenate([labeled_indices, indices_to_acquire])\n",
    "    unlabeled_indices = np.setdiff1d(unlabeled_indices, indices_to_acquire, assume_unique=True)\n",
    "    \n",
    "    # create new subsets and dataloaders\n",
    "    train_subset = Subset(dataset, labeled_indices.tolist())\n",
    "    pool_subset = Subset(dataset, unlabeled_indices.tolist())\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    pool_dataloader = DataLoader(pool_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    return train_dataloader, pool_dataloader, labeled_indices, unlabeled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "131b5a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(model, pool_dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # iterate through pool loader\n",
    "        for inputs, labels in tqdm(pool_dataloader, desc=f\"[Surveying]\"):\n",
    "            # get model predictions, append them to list (num batches, batch size)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            preds = outputs.logits\n",
    "            all_preds.append(preds.cpu())\n",
    "    \n",
    "    # concat predictions from all batches for a single prediction tensor\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1705809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(\n",
    "        n_models, \n",
    "        model_name, \n",
    "        learning_rate,\n",
    "        weight_decay,\n",
    "        epochs,\n",
    "        train_dataloader, \n",
    "        pool_dataloader, \n",
    "        val_dataloader,\n",
    "        patience\n",
    "        ):\n",
    "    \n",
    "    # define list to store predictions as each model is trained then evaluated\n",
    "    ensemble_predictions = []\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        print(f\"\\nTraining Model {i+1}...\")\n",
    "        # set a changing manual seed\n",
    "        torch.manual_seed(i)\n",
    "        torch.cuda.manual_seed(i)\n",
    "\n",
    "        # initialize and train a new model\n",
    "        model = initialize_and_train_new_model(model_name, learning_rate, weight_decay, epochs, train_dataloader, val_dataloader, patience)\n",
    "        \n",
    "        # get model predictions on pool dataloader, append to ensemble predictions list\n",
    "        pool_preds = get_model_predictions(model, pool_dataloader)\n",
    "        ensemble_predictions.append(pool_preds)\n",
    "\n",
    "    # stack ensemble predictions to create tensor of shape (n_models, n_unlabeled_samples)\n",
    "    ensemble_predictions = torch.stack(ensemble_predictions, dim=0)\n",
    "    print(\"Ensemble training complete, submitting predictions for next cycle.\")\n",
    "    # return list of ensemble predictions\n",
    "    return ensemble_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d105347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get acquisition scores (variance) given model predictions\n",
    "def get_acquisition_scores(ensemble_predictions):\n",
    "    # calculate variance for each index\n",
    "    variances = torch.var(ensemble_predictions, dim=0)\n",
    "    # return list of acquisition scores\n",
    "    return variances.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794bb19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# run active learning campaign given num samples, num samples per batch\n",
    "def run_active_learning_campaign(\n",
    "        n_samples,\n",
    "        initial_n_samples,\n",
    "        n_samples_per_batch,\n",
    "        model_name, \n",
    "        learning_rate,\n",
    "        weight_decay,\n",
    "        epochs,\n",
    "        training_pool, \n",
    "        val_dataloader,\n",
    "        patience,\n",
    "        n_models\n",
    "        ):\n",
    "    # initialize index lists\n",
    "    total_pool_size = len(training_pool)\n",
    "    unlabeled_indices = np.arange(total_pool_size)\n",
    "    labeled_indices = np.array([], dtype=np.int64)\n",
    "\n",
    "    ensemble_predictions = None\n",
    "    current_cycle = 1\n",
    "    \n",
    "    while len(labeled_indices) < n_samples and len(unlabeled_indices) > 0:\n",
    "        print(f\"\\nCycle {current_cycle}/{int(np.ceil((n_samples-initial_n_samples)/n_samples_per_batch)) + 1}\\n-------------------------------------------------\")\n",
    "\n",
    "        # on the first cycle, choose random samples of initial_n_samples size\n",
    "        if ensemble_predictions is None:\n",
    "            print(f\"Choosing initial {initial_n_samples} samples randomly...\")\n",
    "            train_dataloader, pool_dataloader, labeled_indices, unlabeled_indices = acquire_new_batch(\n",
    "                training_pool, initial_n_samples, n_samples_per_batch, labeled_indices, unlabeled_indices, acquisition_scores=None\n",
    "            )\n",
    "        # each other time, use the n_samples_per_batch with acquisition scores to select\n",
    "        else:\n",
    "            scores = get_acquisition_scores(ensemble_predictions)\n",
    "            print(f\"Selecting new data points...\")\n",
    "            train_dataloader, pool_dataloader, labeled_indices, unlabeled_indices = acquire_new_batch(\n",
    "                training_pool, initial_n_samples, n_samples_per_batch, labeled_indices, unlabeled_indices, acquisition_scores=scores\n",
    "            )\n",
    "        \n",
    "        if len(unlabeled_indices) == 0:\n",
    "            print(\"Unlabeled pool is empty. Proceeding to final model training.\")\n",
    "            break\n",
    "\n",
    "        print(\"Starting ensemble training and pool evaluation...\")\n",
    "        ensemble_predictions = train_ensemble(n_models, model_name, learning_rate, weight_decay, epochs, train_dataloader, pool_dataloader, val_dataloader, patience)\n",
    "\n",
    "        current_cycle += 1\n",
    "\n",
    "    print(\"\\nActive learning campaign complete.\")\n",
    "    print(f\"Training final model on {len(labeled_indices)} actively selected samples...\")\n",
    "    model, history = initialize_and_train_new_model(model_name, learning_rate, weight_decay, epochs, train_dataloader, val_dataloader, patience, return_history=True)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9977554a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cycle 6/3\n",
      "-------------------------------------------------\n",
      "Selecting new data points...\n",
      "Starting ensemble training and pool evaluation...\n",
      "\n",
      "Training Model 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]:  19%|█▉        | 19/100 [00:09<00:40,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 5 epochs with no improvement.\n",
      "Train Loss: 0.0612 | Val Loss: 0.1342 | SpearmanR: 0.5872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Surveying]: 100%|██████████| 252/252 [00:01<00:00, 149.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]:  11%|█         | 11/100 [00:05<00:44,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 5 epochs with no improvement.\n",
      "Train Loss: 0.1146 | Val Loss: 0.2053 | SpearmanR: 0.5294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Surveying]: 100%|██████████| 252/252 [00:01<00:00, 152.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]:  21%|██        | 21/100 [00:10<00:40,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 5 epochs with no improvement.\n",
      "Train Loss: 0.0607 | Val Loss: 0.1454 | SpearmanR: 0.5548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Surveying]: 100%|██████████| 252/252 [00:01<00:00, 152.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]:  18%|█▊        | 18/100 [00:08<00:39,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 5 epochs with no improvement.\n",
      "Train Loss: 0.0705 | Val Loss: 0.1532 | SpearmanR: 0.5010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Surveying]: 100%|██████████| 252/252 [00:01<00:00, 155.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]:   9%|▉         | 9/100 [00:04<00:45,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 5 epochs with no improvement.\n",
      "Train Loss: 0.1030 | Val Loss: 0.2079 | SpearmanR: 0.4205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Surveying]: 100%|██████████| 252/252 [00:01<00:00, 154.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble training complete, submitting predictions for next cycle.\n",
      "\n",
      "Active learning campaign complete.\n",
      "Training final model on 144 actively selected samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]:  22%|██▏       | 22/100 [00:10<00:38,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 5 epochs with no improvement.\n",
      "Train Loss: 0.0614 | Val Loss: 0.1512 | SpearmanR: 0.5417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"facebook/esm2_t6_8M_UR50D\"\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "EPOCHS = 100\n",
    "PATIENCE = 5\n",
    "N_MODELS = 5\n",
    "\n",
    "model_active, history_active = run_active_learning_campaign(\n",
    "        n_samples = 144,\n",
    "        initial_n_samples = 72,\n",
    "        n_samples_per_batch = 24,\n",
    "        model_name = MODEL_NAME, \n",
    "        learning_rate = LEARNING_RATE,\n",
    "        weight_decay = WEIGHT_DECAY,\n",
    "        epochs = EPOCHS,\n",
    "        training_pool = training_pool, \n",
    "        val_dataloader = val_dataloader,\n",
    "        patience = PATIENCE,\n",
    "        n_models = N_MODELS\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4747322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run standard fine-tuning procedure given num samples\n",
    "def run_standard_finetuning(\n",
    "        n_samples, \n",
    "        model_name, \n",
    "        learning_rate, \n",
    "        weight_decay, \n",
    "        epochs, \n",
    "        training_pool, \n",
    "        val_dataloader,\n",
    "        patience,\n",
    "        ):\n",
    "    # get dataloader of random train data\n",
    "    random_indices = torch.randperm(len(training_pool))[:n_samples].tolist()\n",
    "    train_subset = Subset(training_pool, random_indices)\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # train model\n",
    "    model, history = initialize_and_train_new_model(model_name, learning_rate, weight_decay, epochs, train_dataloader, val_dataloader, patience, return_history=True)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88769ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/miniconda3/envs/ml/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "[Training]:  11%|█         | 11/100 [00:06<00:52,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 5 epochs with no improvement.\n",
      "Train Loss: 0.1754 | Val Loss: 0.1673 | SpearmanR: 0.4616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_standard, history_standard = run_standard_finetuning(\n",
    "        n_samples = 144,\n",
    "        model_name = MODEL_NAME, \n",
    "        learning_rate = LEARNING_RATE,\n",
    "        weight_decay = WEIGHT_DECAY,\n",
    "        epochs = EPOCHS,\n",
    "        training_pool = training_pool, \n",
    "        val_dataloader = val_dataloader,\n",
    "        patience = PATIENCE\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "795f0aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.regression import SpearmanCorrCoef, PearsonCorrCoef, MeanSquaredError\n",
    "\n",
    "def test_model(model, test_dataloader, return_results=False):\n",
    "    # Initialize metrics\n",
    "    spearman = SpearmanCorrCoef().to(device)\n",
    "    pearson = PearsonCorrCoef().to(device)\n",
    "    mse = MeanSquaredError().to(device)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    total_test_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for inputs, labels in tqdm(test_dataloader, desc=\"[Testing]\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            preds = outputs.logits.squeeze()\n",
    "            loss = outputs.loss\n",
    "\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    # Concatenate all predictions and labels from all batches\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # Calculate final metrics\n",
    "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "    spearmanr = spearman(all_preds, all_labels).item()\n",
    "    pearsonr = pearson(all_preds, all_labels).item()\n",
    "    final_mse = mse(all_preds, all_labels).item()\n",
    "\n",
    "    if return_results:\n",
    "        results = {\n",
    "            \"avg_test_loss\": avg_test_loss,\n",
    "            \"spearmanr\": spearmanr,\n",
    "            \"pearsonr\": pearsonr,\n",
    "            \"final_mse\": final_mse\n",
    "        }\n",
    "        return results\n",
    "    else:\n",
    "        # Print the report\n",
    "        print(f\"Spearman's Rho: {spearmanr:.4f}\")\n",
    "        print(f\"Pearson's Rho: {pearsonr:.4f}\")\n",
    "        print(f\"Mean Squared Error (MSE): {final_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a469b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Testing]:   0%|          | 0/33 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Testing]: 100%|██████████| 33/33 [00:00<00:00, 76.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1396\n",
      "Spearman's Rho: 0.5594\n",
      "Pearson's Rho: 0.5615\n",
      "Mean Squared Error (MSE): 0.1396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_model(model_active, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e11a0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Testing]: 100%|██████████| 33/33 [00:00<00:00, 159.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1576\n",
      "Spearman's Rho: 0.5311\n",
      "Pearson's Rho: 0.4833\n",
      "Mean Squared Error (MSE): 0.1576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_model(model_standard, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1d0898f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>spearmanr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.292849</td>\n",
       "      <td>0.237494</td>\n",
       "      <td>0.071654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.260006</td>\n",
       "      <td>0.238660</td>\n",
       "      <td>0.171154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.256437</td>\n",
       "      <td>0.232513</td>\n",
       "      <td>0.250838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.248370</td>\n",
       "      <td>0.258319</td>\n",
       "      <td>0.398959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.253237</td>\n",
       "      <td>0.201234</td>\n",
       "      <td>0.361160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.216148</td>\n",
       "      <td>0.194606</td>\n",
       "      <td>0.412462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.195385</td>\n",
       "      <td>0.186426</td>\n",
       "      <td>0.433561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.155915</td>\n",
       "      <td>0.227530</td>\n",
       "      <td>0.436336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.150174</td>\n",
       "      <td>0.200697</td>\n",
       "      <td>0.455136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.172423</td>\n",
       "      <td>0.166411</td>\n",
       "      <td>0.464650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.126156</td>\n",
       "      <td>0.158798</td>\n",
       "      <td>0.494658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.109330</td>\n",
       "      <td>0.161752</td>\n",
       "      <td>0.509152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.098827</td>\n",
       "      <td>0.171701</td>\n",
       "      <td>0.504017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.088888</td>\n",
       "      <td>0.149980</td>\n",
       "      <td>0.518565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.089300</td>\n",
       "      <td>0.162377</td>\n",
       "      <td>0.516788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.086911</td>\n",
       "      <td>0.155397</td>\n",
       "      <td>0.519147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.079412</td>\n",
       "      <td>0.150213</td>\n",
       "      <td>0.521050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.075060</td>\n",
       "      <td>0.158003</td>\n",
       "      <td>0.525110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.068623</td>\n",
       "      <td>0.159686</td>\n",
       "      <td>0.527794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.070014</td>\n",
       "      <td>0.228334</td>\n",
       "      <td>0.521715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.092133</td>\n",
       "      <td>0.174017</td>\n",
       "      <td>0.533355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.074976</td>\n",
       "      <td>0.144712</td>\n",
       "      <td>0.543452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.061408</td>\n",
       "      <td>0.151193</td>\n",
       "      <td>0.541705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_loss  val_loss  spearmanr\n",
       "0     0.292849  0.237494   0.071654\n",
       "1     0.260006  0.238660   0.171154\n",
       "2     0.256437  0.232513   0.250838\n",
       "3     0.248370  0.258319   0.398959\n",
       "4     0.253237  0.201234   0.361160\n",
       "5     0.216148  0.194606   0.412462\n",
       "6     0.195385  0.186426   0.433561\n",
       "7     0.155915  0.227530   0.436336\n",
       "8     0.150174  0.200697   0.455136\n",
       "9     0.172423  0.166411   0.464650\n",
       "10    0.126156  0.158798   0.494658\n",
       "11    0.109330  0.161752   0.509152\n",
       "12    0.098827  0.171701   0.504017\n",
       "13    0.088888  0.149980   0.518565\n",
       "14    0.089300  0.162377   0.516788\n",
       "15    0.086911  0.155397   0.519147\n",
       "16    0.079412  0.150213   0.521050\n",
       "17    0.075060  0.158003   0.525110\n",
       "18    0.068623  0.159686   0.527794\n",
       "19    0.070014  0.228334   0.521715\n",
       "20    0.092133  0.174017   0.533355\n",
       "21    0.074976  0.144712   0.543452\n",
       "22    0.061408  0.151193   0.541705"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_active_results = pd.DataFrame(history_active)\n",
    "df_standard_results = pd.DataFrame(history_standard)\n",
    "\n",
    "df_active_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918609eb",
   "metadata": {},
   "source": [
    "- More notes\n",
    "    - This simulation plays out in a defined pool of samples, how will this work in the real world, where there's practically infinite options?\n",
    "        - first, you need to define the limits of the space. probably something like a limited number of mutations per sequence, and perhaps a distribution of those mutation numbers in your final set\n",
    "        - boring approach: generate mutants and evaluate ensemble on mutants for a defined amount of time, select the highest variance mutants from that set.\n",
    "        - cool approach: train an adversarial model that learns how to optimally challenge the plm model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23540acd",
   "metadata": {},
   "source": [
    "Anyways, now that we have the active learning loop working, let's start running some experiments to better understand how different parameters effect performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7c2e7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# define the experiment conditions as a list of dicts with the hyperparameters, and \n",
    "# two extra entries, one with the parameter currently changing, and the other with \n",
    "# the local experiment index\n",
    "\n",
    "# define a results list (will be a list of dicts)\n",
    "def run_experiments(experiments, training_pool, val_dataloader, test_dataloader, results_path='active_vs_standard_results.csv'):\n",
    "    results_path = Path(results_path)\n",
    "\n",
    "    # Load existing results if the file exists, otherwise start with a fresh DataFrame.\n",
    "    if results_path.exists():\n",
    "        all_results_df = pd.read_csv(results_path)\n",
    "    else:\n",
    "        all_results_df = pd.DataFrame()\n",
    "\n",
    "    final_results = []\n",
    "    # loop through experiment conditions\n",
    "    for i, exp in enumerate(experiments):\n",
    "        print(f\"\\nEXPERIMENT {i}\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        # run active learning campaign, ignore history for now\n",
    "        params_active = {\n",
    "            \"n_samples\": exp[\"n_samples\"],\n",
    "            \"initial_n_samples\": exp[\"initial_n_samples\"],\n",
    "            \"n_samples_per_batch\": exp[\"n_samples_per_batch\"],\n",
    "            \"model_name\": exp[\"model_name\"], \n",
    "            \"learning_rate\": exp[\"learning_rate\"],\n",
    "            \"weight_decay\": exp[\"weight_decay\"],\n",
    "            \"epochs\": exp[\"epochs\"],\n",
    "            \"patience\": exp[\"patience\"],\n",
    "            \"n_models\": exp[\"n_models\"]\n",
    "        }\n",
    "        model, _ = run_active_learning_campaign(\n",
    "            **params_active, \n",
    "            training_pool=training_pool, \n",
    "            val_dataloader=val_dataloader\n",
    "            )\n",
    "        # run model on test set, returning results\n",
    "        results = test_model(model, test_dataloader, return_results=True)\n",
    "        # add to the results dict the changing var, local experiment idx, the value of the changing var, and training method active\n",
    "        results = {\n",
    "            'changing_var': exp['changing_var'],\n",
    "            'local_exp_idx': exp['local_exp_idx'],\n",
    "            'value': params_active[exp['changing_var']],\n",
    "            'training_method': 'active',\n",
    "            **results\n",
    "        }\n",
    "        # append dict to results list\n",
    "        final_results.append(results)\n",
    "\n",
    "        # run standard fine-tuning, ignore history for now\n",
    "        print(f\"\\nTraining using standard approach, with {exp['n_samples_per_batch']} randomly selected samples...\")\n",
    "        params_standard = {\n",
    "            \"n_samples\": exp[\"n_samples\"],\n",
    "            \"model_name\": exp[\"model_name\"], \n",
    "            \"learning_rate\": exp[\"learning_rate\"],\n",
    "            \"weight_decay\": exp[\"weight_decay\"],\n",
    "            \"epochs\": exp[\"epochs\"],\n",
    "            \"patience\": exp[\"patience\"],\n",
    "        }\n",
    "        model, _ = run_standard_finetuning(\n",
    "            **params_standard, \n",
    "            training_pool=training_pool, \n",
    "            val_dataloader=val_dataloader\n",
    "            )\n",
    "        # run model on test set, returning results\n",
    "        results = test_model(model, test_dataloader, return_results=True)\n",
    "        # add to the results dict the changing var, local experiment idx, the value of the changing var, and training method standard\n",
    "        results = {\n",
    "            'changing_var': exp['changing_var'],\n",
    "            'local_exp_idx': exp['local_exp_idx'],\n",
    "            'value': params_active[exp['changing_var']],\n",
    "            'training_method': 'standard',\n",
    "            **results\n",
    "        }\n",
    "        # append dict to results list\n",
    "        final_results.append(results)\n",
    "\n",
    "        # save to disk each time to save progress\n",
    "        results_df = pd.DataFrame(final_results)\n",
    "        all_results_df = pd.concat([all_results_df, results_df], ignore_index=True)\n",
    "        all_results_df.to_csv(results_path, index=False)\n",
    "        print(f\"Progress for experiment {i} appended to {results_path}\")\n",
    "    return all_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3aefa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_initial_n = [\n",
    "    {\n",
    "        \"changing_var\": \"n_samples\",\n",
    "        \"local_exp_idx\": 0,\n",
    "        \"n_samples\": 48,\n",
    "        \"initial_n_samples\": 24,\n",
    "        \"n_samples_per_batch\": 24,\n",
    "        \"model_name\": \"facebook/esm2_t6_8M_UR50D\", \n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"epochs\": 50,\n",
    "        \"patience\": 10,\n",
    "        \"n_models\": 5\n",
    "    },\n",
    "    {\n",
    "        \"changing_var\": \"n_samples\",\n",
    "        \"local_exp_idx\": 1,\n",
    "        \"n_samples\": 72,\n",
    "        \"initial_n_samples\": 24,\n",
    "        \"n_samples_per_batch\": 24,\n",
    "        \"model_name\": \"facebook/esm2_t6_8M_UR50D\", \n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"epochs\": 50,\n",
    "        \"patience\": 10,\n",
    "        \"n_models\": 5\n",
    "    },\n",
    "    {\n",
    "        \"changing_var\": \"n_samples\",\n",
    "        \"local_exp_idx\": 2,\n",
    "        \"n_samples\": 96,\n",
    "        \"initial_n_samples\": 24,\n",
    "        \"n_samples_per_batch\": 24,\n",
    "        \"model_name\": \"facebook/esm2_t6_8M_UR50D\", \n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"epochs\": 50,\n",
    "        \"patience\": 10,\n",
    "        \"n_models\": 5\n",
    "    },\n",
    "    {\n",
    "        \"changing_var\": \"n_samples\",\n",
    "        \"local_exp_idx\": 3,\n",
    "        \"n_samples\": 144,\n",
    "        \"initial_n_samples\": 24,\n",
    "        \"n_samples_per_batch\": 24,\n",
    "        \"model_name\": \"facebook/esm2_t6_8M_UR50D\", \n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"epochs\": 50,\n",
    "        \"patience\": 10,\n",
    "        \"n_models\": 5\n",
    "    },\n",
    "    {\n",
    "        \"changing_var\": \"n_samples\",\n",
    "        \"local_exp_idx\": 4,\n",
    "        \"n_samples\": 240,\n",
    "        \"initial_n_samples\": 24,\n",
    "        \"n_samples_per_batch\": 24,\n",
    "        \"model_name\": \"facebook/esm2_t6_8M_UR50D\", \n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"epochs\": 50,\n",
    "        \"patience\": 10,\n",
    "        \"n_models\": 5\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be8f485b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cycle 4/4\n",
      "-------------------------------------------------\n",
      "Selecting new data points...\n",
      "Starting ensemble training and pool evaluation...\n",
      "\n",
      "Training Model 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|██████████| 50/50 [00:26<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0043 | Val Loss: 0.1255 | SpearmanR: 0.6381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Surveying]: 100%|██████████| 252/252 [00:00<00:00, 1658.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]:  76%|███████▌  | 38/50 [00:21<00:06,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 10 epochs with no improvement.\n",
      "Train Loss: 0.0178 | Val Loss: 0.1302 | SpearmanR: 0.6569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Surveying]: 100%|██████████| 252/252 [00:01<00:00, 126.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|██████████| 50/50 [00:24<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0086 | Val Loss: 0.1421 | SpearmanR: 0.6605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Surveying]: 100%|██████████| 252/252 [00:01<00:00, 131.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]:  70%|███████   | 35/50 [00:17<00:07,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 10 epochs with no improvement.\n",
      "Train Loss: 0.0259 | Val Loss: 0.1164 | SpearmanR: 0.6514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Surveying]: 100%|██████████| 252/252 [00:01<00:00, 126.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]:  74%|███████▍  | 37/50 [00:21<00:07,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 10 epochs with no improvement.\n",
      "Train Loss: 0.0157 | Val Loss: 0.1068 | SpearmanR: 0.6706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Surveying]: 100%|██████████| 252/252 [00:02<00:00, 116.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble training complete, submitting predictions for next cycle.\n",
      "\n",
      "Active learning campaign complete.\n",
      "Training final model on 144 actively selected samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]:  78%|███████▊  | 39/50 [00:22<00:06,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 10 epochs with no improvement.\n",
      "Train Loss: 0.0182 | Val Loss: 0.1454 | SpearmanR: 0.6433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Testing]: 100%|██████████| 33/33 [00:00<00:00, 116.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training using standard approach, with 24 randomly selected samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]:  86%|████████▌ | 43/50 [00:23<00:03,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 10 epochs with no improvement.\n",
      "Train Loss: 0.0232 | Val Loss: 0.1222 | SpearmanR: 0.6210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Testing]: 100%|██████████| 33/33 [00:00<00:00, 122.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress for experiment 1 appended to active_vs_standard_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>changing_var</th>\n",
       "      <th>local_exp_idx</th>\n",
       "      <th>value</th>\n",
       "      <th>training_method</th>\n",
       "      <th>avg_test_loss</th>\n",
       "      <th>spearmanr</th>\n",
       "      <th>pearsonr</th>\n",
       "      <th>final_mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>initial_n_samples</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>active</td>\n",
       "      <td>0.152872</td>\n",
       "      <td>0.615488</td>\n",
       "      <td>0.619794</td>\n",
       "      <td>0.152872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>initial_n_samples</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>standard</td>\n",
       "      <td>0.145488</td>\n",
       "      <td>0.505765</td>\n",
       "      <td>0.539804</td>\n",
       "      <td>0.145488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>initial_n_samples</td>\n",
       "      <td>2</td>\n",
       "      <td>72</td>\n",
       "      <td>active</td>\n",
       "      <td>0.114177</td>\n",
       "      <td>0.666418</td>\n",
       "      <td>0.684209</td>\n",
       "      <td>0.114177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>initial_n_samples</td>\n",
       "      <td>2</td>\n",
       "      <td>72</td>\n",
       "      <td>standard</td>\n",
       "      <td>0.119180</td>\n",
       "      <td>0.631235</td>\n",
       "      <td>0.642157</td>\n",
       "      <td>0.119180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        changing_var  local_exp_idx  value training_method  avg_test_loss  \\\n",
       "0  initial_n_samples              1     48          active       0.152872   \n",
       "1  initial_n_samples              1     48        standard       0.145488   \n",
       "2  initial_n_samples              2     72          active       0.114177   \n",
       "3  initial_n_samples              2     72        standard       0.119180   \n",
       "\n",
       "   spearmanr  pearsonr  final_mse  \n",
       "0   0.615488  0.619794   0.152872  \n",
       "1   0.505765  0.539804   0.145488  \n",
       "2   0.666418  0.684209   0.114177  \n",
       "3   0.631235  0.642157   0.119180  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiments(experiments_initial_n, training_pool, val_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d71c18d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
