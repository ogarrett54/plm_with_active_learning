{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a496b305",
   "metadata": {},
   "source": [
    "# Active Fine-tuning of PLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743064de",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Trial three approaches for active fine-tuning of ESM2 on our Pikh1 HMA data\n",
    "    1. train an ensemble of models and use the mean and variance of their predictions to guide learning\n",
    "    2. use dropout layer during prediction, find mean and variance of predictions\n",
    "    3. mean variance estimation fine-tuning to predict two values with gaussian negative log-likelihood loss\n",
    "- Compare data efficiency between each method (that is, how many training labels are needed to achieve the same spearman r on a universal test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4471d3",
   "metadata": {},
   "source": [
    "## Prepare data splits\n",
    "Starting data is from FACS of surface displayed Pikh1 HMA variants when exposed to 1 uM AVR-PikC. There are 3960 labels in this dataset, each with a full sequence and an enrichment score, roughly correlated with binding affinity. We'll load these into a torch dataset, then split into 80% train, 10% val, and 10% test. The training data will be what we use for all active learning loops, validated on the val set. The test set will be used as a universal final test set for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19d00765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa_sequence</th>\n",
       "      <th>enrichment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...</td>\n",
       "      <td>1.468796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GLKRIIVIKVAREGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...</td>\n",
       "      <td>1.415944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GLKRIIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...</td>\n",
       "      <td>1.389615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...</td>\n",
       "      <td>1.359651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...</td>\n",
       "      <td>1.343857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         aa_sequence  enrichment_score\n",
       "0  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...          1.468796\n",
       "1  GLKRIIVIKVAREGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...          1.415944\n",
       "2  GLKRIIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...          1.389615\n",
       "3  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...          1.359651\n",
       "4  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...          1.343857"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('avrpikC_full.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9613c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = df.aa_sequence\n",
    "scores = df.enrichment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afccfeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class BindingDataset(Dataset):\n",
    "    def __init__(self, sequences, scores):\n",
    "        # make sure sequence and scores have the same length\n",
    "        assert len(sequences) == len(scores), f\"Sequences and scores must be of the same length.\\nNumber of sequences: {len(sequences)}\\nNumber of scores: {len(scores)}\"\n",
    "        self.sequences = sequences\n",
    "        self.scores = scores\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = torch.tensor(self.scores[idx], dtype=torch.float)\n",
    "\n",
    "        # tokenize the sequence\n",
    "        tokenized = self.tokenizer(\n",
    "            sequence,\n",
    "            max_length=80, # 78 residues + 2 extra tokens\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # return input_ids: attention masks, removing the batch dimension\n",
    "        inputs = {key: val.squeeze(0) for key, val in tokenized.items()}\n",
    "\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99740828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "full_dataset = BindingDataset(sequences, scores)\n",
    "\n",
    "# split the data into train, val, and test sets\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = int(0.1 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# define dataloaders for val and test sets, train will be defined later for subsets\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3cdc9b",
   "metadata": {},
   "source": [
    "## The active learning loop\n",
    "1. Initialize model 1: pretrained esm2 with a randomized binding head\n",
    "2. Train on an initial batch of 24 samples (test random and zero-shot predictions).\n",
    "    - Initial hyperparameters (note that during an actual active learning run, you can't change this in the middle of the campaign)\n",
    "        - optimizer: AdamW\n",
    "        - learning rate: 2e-5\n",
    "        - weight decay: 0.01\n",
    "        - early stopping\n",
    "3. Evaluate on validation set\n",
    "4. Evalutate on the rest of the available training pool\n",
    "5. Initialize model 2\n",
    "6. Repeat 2-5 until 5 models have been trained and used to evalutate\n",
    "7. Calculate variance of model predictions on each sequence in the training pool.\n",
    "8. Select the next 24 sequences with the highest variance.\n",
    "9. Reinitialize model 1 with pretrained esm2 and a freshly randomized binding head.\n",
    "10. Repeat ensemble training, evaluation, and acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eff210",
   "metadata": {},
   "source": [
    "- Notes about alternative approaches for the baseline fine-tuning approach:\n",
    "    - Fine-tuning ESM2 is still under active research, and recent literature suggests that just using the CLS token for mutant effects may not be the best approach\n",
    "    - [ESM Effect](https://www.biorxiv.org/content/10.1101/2025.02.03.635741v1.full.pdf) details a framework for using the mutant token vectors as the basis for predicting mutant effects.\n",
    "        - Here, they use 35M model with last two layers unfrozen. They use the mutant tokens to predict mutant effects. They have a prediction head composed of two linear layers. However, their datasets are of deep mutational scans, so only one mutation. I would have to adapt this for higher numbers of mutations.\n",
    "            - to adapt it for a higher number of mutations, I could pool, concatenate, or use an attention mechanism\n",
    "- For now, I'm going to set up the active training loop using the standard CLS token approach (aka, what the AutoModelForSequenceClassification class does from the transformers library), but I will need to separately experiment with other approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f789004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import AdamW\n",
    "import tqdm\n",
    "from torchmetrics.regression import SpearmanCorrCoef\n",
    "\n",
    "MODEL_NAME = \"facebook/esm2_t12_8M_UR50D\"\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "ES_PATIENCE = 8\n",
    "\n",
    "def initialize_HF_ESM2():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels = 1)\n",
    "    # loss_fn = MSELoss() HuggingFace automatically handles the loss\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    spearman = SpearmanCorrCoef()\n",
    "    return model, optimizer, spearman #,loss_fn\n",
    "\n",
    "def train_step(model, optimizer, train_dataloader):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for inputs, labels in tqdm(train_dataloader, desc=f\"[Training]\"):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    return avg_train_loss\n",
    "\n",
    "def val_step(model, val_dataloader, spearman):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for inputs, labels in tqdm(val_dataloader, desc=f\"[Validation]\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            preds = outputs.logits\n",
    "            loss = outputs.loss\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "    spearmanr = spearman(all_preds, all_labels).item()\n",
    "\n",
    "    return avg_val_loss, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "143eab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs, train_dataloader, val_dataloader):\n",
    "    model, optimizer, spearman = initialize_HF_ESM2()\n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}/{epochs}\\n----------------------------')\n",
    "        train_loss = train_step(model, optimizer, train_dataloader)\n",
    "        val_loss, spearmanr = val_step(model, val_dataloader, spearman)\n",
    "        print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | SpearmanR: {spearmanr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce232e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get initial train batch given dataset, num samples\n",
    "    # get the total number of samples in the training pool\n",
    "    # list out their indicies\n",
    "    # randomly selected the given number of those\n",
    "    # store unlabeled indices\n",
    "    # make subsets\n",
    "    # return train dataloader, pool dataloader, labeled indicies, and unlabeled indicies\n",
    "\n",
    "# get new train batch given dataset, acquisition scores, currently labeled, unlabeled indicies, num samples\n",
    "    # get the indicies of the top acquisition scores (num of samples)\n",
    "        # handle when remaining samples < num samples\n",
    "    # use these to find the indicies that map back to the original dataset\n",
    "    # add these to the currently labeled indicies, remove them from unlabeled indicies\n",
    "    # make subsets\n",
    "    # return train dataloader, pool dataloader, labeled indicies, and unlabeled indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b5a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model predictions on unlabeled dataset given pool dataloader\n",
    "    # set the model to eval mode\n",
    "    # define list to store predictions\n",
    "    # set inference mode\n",
    "        # iterate through pool loader\n",
    "            # get model predictions, append them to list (num batches, batch size)\n",
    "    # concat predictions from all batches for a single prediction tensor\n",
    "    # return full prediction tensor\n",
    "\n",
    "# train ensemble given training subset, pool subset, validation set, and number of models\n",
    "    # define list to store ensemble predictions\n",
    "    # for i in number of models\n",
    "        # set manual seed i\n",
    "        # initialize model\n",
    "        # run training loop\n",
    "        # get model predictions on pool subset, append to list \n",
    "    # return list of ensemble predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d105347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get acquisition scores (variance) given model predictions\n",
    "    # calculate variance for each index\n",
    "    # return list of acquisition scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794bb19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run active learning campaign given num samples, num samples per batch\n",
    "    # calculate num cycles\n",
    "    # for each cycle\n",
    "        # if first cycle\n",
    "            # get initial train batch\n",
    "        # else\n",
    "            # get acquisition scores\n",
    "            # get new train batch\n",
    "        # train ensemble, storing predictions\n",
    "    # train final model using last train dataloader\n",
    "    # return final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4747322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run standard fine-tuning procedure given num samples\n",
    "    # get dataloader of random train data\n",
    "    # train model\n",
    "    # return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918609eb",
   "metadata": {},
   "source": [
    "- More notes\n",
    "    - This simulation plays out in a defined pool of samples, how will this work in the real world, where there's practically infinite options?\n",
    "        - first, you need to define the limits of the space. probably something like a limited number of mutations per sequence, and perhaps a distribution of those mutation numbers in your final set\n",
    "        - boring approach: generate mutants and evaluate ensemble on mutants for a defined amount of time, select the highest variance mutants from that set.\n",
    "        - cool approach: train an adversarial model that learns how to optimally challenge the plm model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc96c8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plm_active_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
