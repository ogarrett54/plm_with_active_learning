{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a496b305",
   "metadata": {},
   "source": [
    "# Active Fine-tuning of PLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743064de",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Trial three approaches for active fine-tuning of ESM2 on our Pikh1 HMA data\n",
    "    1. train an ensemble of models and use the mean and variance of their predictions to guide learning\n",
    "    2. use dropout layer during prediction, find mean and variance of predictions\n",
    "    3. mean variance estimation fine-tuning to predict two values with gaussian negative log-likelihood loss\n",
    "- Compare data efficiency between each method (that is, how many training labels are needed to achieve the same spearman r on a universal test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4471d3",
   "metadata": {},
   "source": [
    "## Prepare train test split\n",
    "Starting data is from FACS of surface displayed Pikh1 HMA variants when exposed to 1 uM AVR-PikC. There are 3960 labels in this dataset, each with a full sequence and an enrichment score, roughly correlated with binding affinity. We'll load these into a torch dataset then reserve 10% as a universal test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d00765",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
