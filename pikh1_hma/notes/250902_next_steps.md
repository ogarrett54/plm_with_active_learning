# 2025/09/02: Next Steps

## Where I've been

The basic objective of the initial phase of this project was to identify a method to more efficiently full-fine-tune ESM2 8M. 

First, I tested a basic ensemble method where I trained 5 independent models initialized with different random seeds, collected their predictions on the available training pool, and selected the samples with the highest variance in the ensemble predictions. This method performed demonstrably *worse* than randomly selecting the same amount of samples.

Next, I tested the Monte Carlo Dropout method, which is essentially where you train one model, but when surveying the training pool, turn on dropout and do quite a few predictions. ESM2 does not natively have dropout enabled at any point but they layers are present. So, I turned all the dropout layers on with p=0.1, and did 24 predictions on each sample in the training pool, again selecting the samples with the highest variance. This method didn't perform substantially worse, but on par with random selection. However, it did have the weird behavior of increasing variance over time, suggesting the variance measurement wasn't actually a great measurement, and the model was just getting more confused over time.

My third test was a repeat of the ensemble, but this time using bootstrapping to make the models more different from each other, with the idea of making the variance more representative of model perplexity. For this approach, I would sample from the training pool, then train 5 different models with random initializations on 90% of that sample. I would collect their predictions and again, I selected the next batch of samples by the one that had the highest variance. This method performed mostly worse, but it ended up converging near the end such that random selection and this selection method weren't substantially different from one another. 

For the fourth test, I suspected things were performing worse or not better because just choosing the highest variance samples would be more likely to choose samples that are just noisier due to aleatoric (irreducible, inherent to the measurement) rather than epistemic (lack of knowledge) uncertainty. To rectify this, I wondered if having diverse batches that were only biased toward higher variance would yield better results. To test this, I tried random sampling from the top 25% variance samples (using bootstrapped ensemble method) and using the fine-tuned model embeddings to maximize inter-sample embedding distance in the top 50% of variance samples, neither of which helped. I also tested the embedding distance without regard to model variance, which also didn't help.

So, at this point, I figured I need to do some diagnostics. 

## Diagnostics

As of today, I've already performed one diagnostic. I wondered if my approaches were biasing my samples toward those with lower initial read counts. Samples with lower library read counts are inherently going to have more aleatoric uncertainty in their enrichment scores. The most direct way to test this would be to collect the samples as I run the active learning campaign and trace back their read counts. However, that was looking to be more complex than another method which I implemented to see if its even worth doing this. That is, I did standard fine-tuning runs with 256 samples or less, selected from pools of training samples that had initial read counts ranging from 10-24. If my hypothesis was correct, I would have expected to see an increase in model performance as the read counts increased. This did not occur.

Now that all of the low-hanging fruit are out of the way, I need to do some more comprehensive diagnostic experiments. The first thing to do is assay uncertainty calibration. Conceptually, this is straight-forward: measure the correlation between my uncertainty measurement (variance) and model residuals. If uncertainty is calibrated, you would expect positive correlation between model uncertainty and the actual prediction error. In code, this will be a little tricky. I'll have to link the variances back to the original samples, then have the properly trained active learning model make its predictions on those and compute the residuals. I'll also have to see how this changes cycle by cycle. If it turns out that uncertainty is not calibrated, I'll need to find other methods of measuring model uncertainty.
    - For each cycle, I already save variances. The indices of variances are matched up with the indices of unlabeled samples, which itself stores indices of the original dataset. The properly trained model should be made to predict the values in the training pool and residuals calculated on the spot and stored in dataframe. Follow this, go into the get variances function and append those to the dataframe. The orders *should* be correct if I make sure I'm not shuffling. I guess it's really that tricky. I'll be mostly copy-pasting the get pool predictions function, but I'll subtract the prediction from the label, take the absolute value, then append that to a list that I'll return

The next diagnostic experiment I will need to do is check for selection bias toward extremes. Models get good spearman scores when they are trained on data spanning the distribution. If active learning is biasing toward really high or really low enrichment scores, it won't know what to do with the samples in between, reducing performance. If this is the case, my acquisition function should ensure an even distribution of enrichment scores in the next batch.
    - This one should also be pretty easy. I'll run a modified get pool predictions function on the train dataloader to get predictions and I'll return both the predicitons and labels.